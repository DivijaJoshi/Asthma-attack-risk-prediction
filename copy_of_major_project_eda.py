# -*- coding: utf-8 -*-
"""Copy of Major_Project_EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rs6EI9DcGqLc6RFIhKCJZi1cfcKGXEr1

#**EDA Of data with reduced dataset and balanced classes**
##**Overview**
*The reduced dataset consists of 21 columns of patient history with factors that directly or indirectly contributed to  asthma attacks.*

- diseases            
- medication          
- prev_sleep_dist     
- prev_attack         
- prev_doc_visit     
- prev_health_cond    
- control_asthma      
- sensitivity         
- smoke               
- work                
- cough               
- pefr                
- ext_trig            
- chest_tig           
- medc_usage          
- Humidity            
- AQI                 
- asthma_attack

---

***Loading both the datasets here:***
"""

# Importing required libraries
import pandas as pd
import numpy as np

# Loading the datasets
data1 = pd.read_csv('./data with reduced dataset and balanced classes.csv')

# Preview the datasets
print("Data 1:")
data1.head()

"""# Check for missing values

*   Dataset 1 has no missing values




"""

# Check for missing values
print("Missing values in Data 1:")
print(data1.isnull().sum())

"""##**Statistics of both datasets**
This method calculates and displays summary statistics for each numerical column in the data1 DataFrame. The statistics typically include:

- count: The number of non-null entries.
- mean: The average value.
- std: The standard deviation, which measures the amount of variation or dispersion in the dataset.
- min: The minimum value.
- 25%: The 25th percentile (the first quartile), indicating that 25% of the data falls below this value.
- 50%: The median or 50th percentile, which is the middle value.
- 75%: The 75th percentile (the third quartile), indicating that 75% of the data falls below this value.
- max: The maximum value.
"""

# Basic statistics of both datasets
print("Data 1 statistics:")
print(data1.describe())

import seaborn as sns
import matplotlib.pyplot as plt

# Visualize distributions of key features
plt.figure(figsize=(5, 3))
sns.histplot(data1['diseases'], kde=True)  # Replace with actual feature names
plt.title('Distribution of Feature1')
plt.show()

"""##Summary:
The code generates a correlation heatmap for the data1 dataset, which visually represents the correlation coefficients between numeric features.
###Purpose:
- To visualize the strength and direction of relationships between features in data1.
- Correlation Coefficients: Values range from -1 (perfect negative correlation) to 1 (perfect positive correlation), with 0 indicating no correlation.
- Annotations: The heatmap displays correlation values in each cell for easy interpretation.
- Color Mapping: Positive correlations are shown in warm colors (e.g., red), while negative correlations are depicted in cool colors (e.g., blue).
"""

# Correlation heatmap for Data 1
plt.figure(figsize=(12, 8))
sns.heatmap(data1.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap for Data 1')
plt.show()

"""##Purpose:To handle missing values (NaNs) in the data1 and data2 DataFrames.
- **Method Used:** The ffill method, or "forward fill," fills missing values with the last valid observation in the dataset.
###How It Works:
When a cell contains a NaN (missing value), it is replaced with the most recent non-NaN value that precedes it in the same column.
###Example:
If you have a column with values [1, NaN, NaN, 4], after applying fillna(method='ffill'), it will become [1, 1, 1, 4].
"""

data1.fillna(method='ffill', inplace=True)

"""##Summary:
**Purpose**: To visualize the distribution of numeric features in data1, providing insights into their characteristics and data spread.

**Result:**The output will be a grid of histogram plots, each displaying the distribution of a different numeric column in data1, along with a KDE line that helps visualize the shape of the distribution. This is useful for understanding the central tendency, spread, and potential outliers in the data.

***Kernel Density Estimation (KDE) is a non-parametric way to estimate the probability density function (PDF) of a random variable.***
"""

# Plot distribution for numeric columns in Data 1
numeric_columns1 = data1.select_dtypes(include=[np.number]).columns

# Determine number of rows and columns for subplots
n_cols = 2  # Set to 2 or 3 depending on how you want to arrange the grid
n_rows = (len(numeric_columns1) + n_cols - 1) // n_cols  # This ensures enough rows for all columns

plt.figure(figsize=(15, 5 * n_rows))  # Adjust the figure size based on number of rows
for i, column in enumerate(numeric_columns1, 1):
    plt.subplot(n_rows, n_cols, i)
    sns.histplot(data1[column], kde=True)
    plt.title(f'Distribution of {column}')
plt.tight_layout()
plt.show()

"""###Purpose:
- The x-axis represents the features with significant correlations.
- while the y-axis shows the correlation coefficients.
The plot helps to easily identify which features are positively correlated with asthma attacks and, providing insights for further analysis or model development.
- This ultimately allows you to assess which features in the data1 dataset are most relevant to asthma attacks based on their correlation coefficients.


---


###Result:
###These factors are more likely relevent to asthma attacks from Dataset 1:

- work                0.524211
- cough               0.466528
- chest_tig           0.418074
- medc_usage          0.250877
- prev_attack         0.245643
- smoke               0.236144
- fam_asthma          0.218948
- AQI                 0.218017
"""

# Feature selection based on correlation with asthma attack indicator (Assuming 'asthma_attack' column exists)
correlation_data1 = data1.corr()['asthma_attack'].sort_values(ascending=False)
print(correlation_data1)

# Visualize the strongest correlations
strong_corr_data1 = correlation_data1[correlation_data1 > 0.1]  # Adjust threshold as needed
plt.figure(figsize=(10, 6))
strong_corr_data1.plot(kind='bar', color='skyblue')
plt.title('Strong Correlations with Asthma Attacks (Data 1)')
plt.show()

"""**MODEL** **TRAINING**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

relevant_features = ['work', 'cough', 'chest_tig', 'medc_usage', 'prev_attack', 'smoke', 'fam_asthma', 'AQI']
X = data1[relevant_features]
y = data1['asthma_attack']

X.head(5)

y

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardizing the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'Support Vector Machine': SVC()
}

# Dictionary to store the performance of each model
model_performance = {}

# Training, evaluating and storing the results for each model
for name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)
    # Predict on the test set
    y_pred = model.predict(X_test)
    # Calculate performance metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    # Store the results
    model_performance[name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1
    }

# Convert the performance dictionary into a DataFrame for better readability
performance_df = pd.DataFrame(model_performance).T
print("Model Performance:")
print(performance_df)

# Select the best model (Random Forest) and evaluate it on the test set
best_model = RandomForestClassifier()
best_model.fit(X_train, y_train)
y_test_pred = best_model.predict(X_test)

# Evaluate the best model's performance on the test set
accuracy_test = accuracy_score(y_test, y_test_pred)
precision_test = precision_score(y_test, y_test_pred)
recall_test = recall_score(y_test, y_test_pred)
f1_test = f1_score(y_test, y_test_pred)

print("\nBest Model Performance on Test Set:")
print(f"Accuracy: {accuracy_test:.4f}")
print(f"Precision: {precision_test:.4f}")
print(f"Recall: {recall_test:.4f}")
print(f"F1 Score: {f1_test:.4f}")

# Optional: Confusion Matrix and Classification Report
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_test_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_test_pred))

# Function to take relevant user input and make a prediction
def predict_asthma_attack():
    # Take input for the most relevant features
    work = float(input("Work environment (float value): "))
    cough = float(input("Cough level (float value): "))
    chest_tig = float(input("Chest tightness (float value): "))
    medc_usage = float(input("Medication usage (float value): "))
    prev_attack = float(input("Previous asthma attacks (float value): "))
    smoke = float(input("Smoking status (float value): "))
    fam_asthma = float(input("Family history of asthma (float value): "))
    AQI = float(input("Air Quality Index (float value): "))

    # Combine the input into a dictionary to create a DataFrame
    user_input_dict = {
        'work': [work],
        'cough': [cough],
        'chest_tig': [chest_tig],
        'medc_usage': [medc_usage],
        'prev_attack': [prev_attack],
        'smoke': [smoke],
        'fam_asthma': [fam_asthma],
        'AQI': [AQI]
    }

    # Create a DataFrame from the input
    user_input_df = pd.DataFrame(user_input_dict)

    # Apply the same scaling that was used during training
    user_input_scaled = scaler.transform(user_input_df)

    # Predict the asthma attack outcome
    prediction = best_model.predict(user_input_scaled)

    # Output the prediction
    if prediction == 1:
        print("The model predicts an asthma attack (1).")
    else:
        print("The model predicts no asthma attack (0).")

# Call the function to get input and predict
predict_asthma_attack()